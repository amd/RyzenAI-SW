<table class="sphinxhide" width="100%">
 <tr width="100%">
    <td align="center"><img src="https://raw.githubusercontent.com/Xilinx/Image-Collateral/main/xilinx-logo.png" width="30%"/><h1> Ryzenâ„¢ AI Text Classification </h1>
    </td>
 </tr>
</table>

# Running DistilBERT on Ryzen AI

This Ryzen AI example demonstrates how to run the ``distilbert-base-uncased-finetuned-sst-2-english`` model on an NPU. This model is a fine-tuned checkpoint of **DistilBERT-base-uncased**, trained on the **SST-2** dataset for sentiment analysis. Developed by Hugging Face, it can be used for both single-label and multi-label classification.

For more details, refer to the Hugging Face Model Card: [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english)

## Setup Instructions

Activate the conda environment created by the RyzenAI installer

```bash
conda activate <env_name>
```

## Deployment Steps

The following steps outline how to deploy the model on an NPU:

- Download the model from Hugging Face and convert it to ONNX (Opset 17).
- Quantize the model to BF16 using the AMD Quark Quantizer.
- Compile and run the model on an NPU using ONNX Runtime with the Vitis AI Execution Provider.

Details for each step are provided below.

### 1. ONNX Conversion

To download the model and convert it to ONNX, run the following script:

```
python pt_to_onnx.py --output_dir model
```

The script ``pt_to_onnx.py`` downloads the model from the Hugging Face checkpoint and converts it to ONNX format with Opset 17 using the ``torch.onnx.export`` API.

After running this command, the converted model will be saved as: ``model/distilbert-base-uncased-finetuned-sst-2-english.onnx``


### 2. BF16 Quantization

To quantize the FP32 model to BF16, run the following script:

```
python quantize_to_bf16.py --model_path "model/distilbert-base-uncased-finetuned-sst-2-english.onnx" --output_dir "model"
```

During quantization, AMD Quark converts both weights and activations to the BF16 format. In this example, random data is used for the calibration process.

After quantization, the BF16 model is saved as: ``model/distilbert-base-uncased-finetuned-sst-2-english_bf16.onnx``



### 3. Running Inference

To run inference, execute the following script:

```
python run_inference.py
```

The ONNX Runtime Vitis AI Execution Provider compiles and runs the model on an NPU. A BF16-specific configuration file is passed by the ``config_file`` provider option.

The first-time compilation may take some time, but the compiled model is cached in a directory specified by the ``cache_dir`` and ``cache_key`` provider options.
Subsequent runs bypass the compilation process and directly execute the model on the NPU.

After a successful run, the model will be deployed on the NPU and provide sentiment classification for input texts used in the script

```
**********
Prompt: Hello, my dog is cute  
Text classification: POSITIVE  
**********  
Prompt: Stop talking to me  
Text classification: NEGATIVE  
**********  
Prompt: That painting is ugly  
Text classification: NEGATIVE  
**********  
Prompt: Life is beautiful  
Text classification: POSITIVE  
**********
```
