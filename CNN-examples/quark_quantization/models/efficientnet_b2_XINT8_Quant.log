The configuration of the quantization is Config(global_quant_config=QuantizationConfig(calibrate_method=<PowerOfTwoMethod.MinMSE: 1>, quant_format=<QuantFormat.QDQ: 1>, activation_type=<QuantType.QUInt8: 1>, weight_type=<QuantType.QInt8: 0>, input_nodes=[], output_nodes=[], op_types_to_quantize=[], nodes_to_quantize=[], extra_op_types_to_quantize=[], nodes_to_exclude=[], subgraphs_to_exclude=[], specific_tensor_precision=False, execution_providers=['CPUExecutionProvider'], per_channel=False, reduce_range=False, optimize_model=True, use_dynamic_quant=False, use_external_data_format=False, convert_fp16_to_fp32=False, convert_nchw_to_nhwc=False, include_sq=False, include_rotation=False, include_cle=True, include_auto_mp=False, include_fast_ft=False, enable_npu_cnn=True, enable_npu_transformer=False, debug_mode=False, crypto_mode=False, print_summary=True, ignore_warnings=True, log_severity_level=1, extra_options={'ActivationSymmetric': True}))
[QUARK_INFO]: Time information:
2025-10-14 17:04:10.060790
[QUARK_INFO]: OS and CPU information:
                                        system --- Windows
                                          node --- XSJSTRXHPOMNI01
                                       release --- 11
                                       version --- 10.0.26100
                                       machine --- AMD64
                                     processor --- AMD64 Family 26 Model 36 Stepping 0, AuthenticAMD
[QUARK_INFO]: Tools version information:
                                        python --- 3.12.11
                                          onnx --- 1.18.0
                                   onnxruntime --- 1.23.0.dev20250928
                                    quark.onnx --- 0.10+db671e3+db671e3
[QUARK_INFO]: Quantized Configuration information:
                                   model_input --- perf_models\efficientnet_b2.onnx
                                  model_output --- perf_models\efficientnet_b2_XINT8.onnx
                       calibration_data_reader --- <utils.ImageDataReader object at 0x0000016FA86A6900>
                         calibration_data_path --- None
                                  quant_format --- QDQ
                                   input_nodes --- []
                                  output_nodes --- []
                          op_types_to_quantize --- []
                    extra_op_types_to_quantize --- []
                                   per_channel --- False
                                  reduce_range --- False
                               activation_type --- QUInt8
                                   weight_type --- QInt8
                             nodes_to_quantize --- []
                              nodes_to_exclude --- []
                          subgraphs_to_exclude --- []
                                optimize_model --- True
                      use_external_data_format --- False
                              calibrate_method --- PowerOfTwoMethod.MinMSE
                           execution_providers --- ['CPUExecutionProvider']
                                enable_npu_cnn --- True
                        enable_npu_transformer --- False
                     specific_tensor_precision --- False
                                    debug_mode --- False
                          convert_fp16_to_fp32 --- False
                          convert_nchw_to_nhwc --- False
                                   include_cle --- True
                                    include_sq --- False
                              include_rotation --- False
                               include_fast_ft --- False
                                 extra_options --- {'ActivationSymmetric': True}
+---------------------------------------------------------------+
| Op Type              | Float Model                            |
|----------------------+----------------------------------------|
| Conv                 | 115                                    |
| Sigmoid              | 92                                     |
| Mul                  | 92                                     |
| GlobalAveragePool    | 24                                     |
| Add                  | 16                                     |
| Flatten              | 1                                      |
| Gemm                 | 1                                      |
|----------------------+----------------------------------------|
| Quantized model path | perf_models\efficientnet_b2_XINT8.onnx |
+---------------------------------------------------------------+
+--------------------------------------------------------+
| Op Type           | Activation | Weights   | Bias      |
|-------------------+------------+-----------+-----------|
| Conv              | UINT8(115) | INT8(115) | INT8(115) |
| Mul               | UINT8(92)  |           |           |
| AveragePool       | UINT8(8)   |           |           |
| GlobalAveragePool | UINT8(24)  |           |           |
| Add               | UINT8(16)  |           |           |
| Flatten           | UINT8(1)   |           |           |
| Gemm              | UINT8(1)   | INT8(1)   | INT8(1)   |
| HardSigmoid       | UINT8(92)  |           |           |
+--------------------------------------------------------+
Model Size:
Float32 model size: 34.72 MB
Int8 quantized model size: 9.40 MB
Model Accuracy:
Float32 model accuracy: Top1 0.779, Top5 0.940 
Using TXN FORMAT 0.1
[Vitis AI EP] No. of Operators :   CPU   184    NPU  1135 VITIS_EP_CPU   186 
[Vitis AI EP] No. of Subgraphs :   NPU    93 Actually running on NPU     93
Int8 quantized model accuracy: Top1 0.001, Top5 0.010 
C:\Users\dwchenna\github\amd_repo\RyzenAI-SW\tutorial\quark_quantization
