{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a5cbbb-8d20-49ae-9dcc-90ed3f623793",
   "metadata": {},
   "source": [
    "### Classification example inference with Ryzen AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777199ee-9183-40bb-8355-1fb470552f5f",
   "metadata": {},
   "source": [
    "This example demonstrates the 5 steps of classification model inference on the embedded Neural Processing Unit (NPU) in your AMD Ryzen AI enabled PC. The steps are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba8184db-eee4-499b-b5c0-c494e643bd06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: Checking custom ops library ...\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: The CPU version of custom ops library already exists.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Checked custom ops library.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import subprocess\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import onnx\n",
    "import shutil\n",
    "import time \n",
    "from timeit import default_timer as timer\n",
    "from quark.onnx import ModelQuantizer  \n",
    "from quark.onnx.quantization.config import Config, get_default_config  \n",
    "from utils_custom import ImageDataReader, evaluate_onnx_model \n",
    "import json  \n",
    "import shutil\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470d814-78bb-481e-b512-d431d267c003",
   "metadata": {},
   "source": [
    "#### 1. Get Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd67743-027b-4988-a467-b4a4a173259e",
   "metadata": {},
   "source": [
    "Here, we'll use the resnet50 model as an example. You may choose any classification models train with Imagenet from torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "738304be-b948-4f34-a69d-ba4d2277e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Model Setup ---------------- #\n",
    "\n",
    "# Define directories\n",
    "models_dir = \"models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "model = torchvision.models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "\n",
    "# Save the model\n",
    "model.to(\"cpu\")\n",
    "torch.save(model, os.path.join(models_dir, \"resnet50.pt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce87c669-4f5c-433a-bb37-676b40d4640f",
   "metadata": {},
   "source": [
    "#### 2. Export to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07e198-29b2-463a-b9df-10266d390249",
   "metadata": {},
   "source": [
    "The model inference with Ryzen AI is based on onnxruntime. The following code is used for exporting a PyTorch model to the ONNX (Open Neural Network Exchange) format. The ONNX file is needed to use the AMD Quark Quantizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73d3bad8-b5ea-4c08-bea6-5c92c973055b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model exported to ONNX at: models\\resnet50.onnx\n"
     ]
    }
   ],
   "source": [
    "# Export model to ONNX\n",
    "dummy_inputs = torch.randn(1, 3, 224, 224)\n",
    "input_names = ['input']\n",
    "output_names = ['output']\n",
    "dynamic_axes = {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "tmp_model_path = os.path.join(models_dir, \"resnet50.onnx\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_inputs,\n",
    "    tmp_model_path,\n",
    "    export_params=True,\n",
    "    opset_version=13,  # Recommended opset\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    dynamic_axes=dynamic_axes,\n",
    ")\n",
    "\n",
    "print(f\"✅ Model exported to ONNX at: {tmp_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6bfea-4270-4d2d-98cd-0a5497224265",
   "metadata": {},
   "source": [
    "#### 3. Quantize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d5103-9e41-43c0-8298-fc4e5bd8d6d5",
   "metadata": {},
   "source": [
    "Using the AMD Quark Quantizer and providing the newly exported ONNX model, we'll quantize the model. The quantization progress will need the calibration data from Imagenet. Download the data from [here](https://huggingface.co/datasets/imagenet-1k/tree/main/data) to download it.\n",
    "You need to register on Hugging Face and download the following file:\n",
    "**val_images.tar.gz**.\n",
    "This file contains a subset of ImageNet images used specifically for calibration.\n",
    "\n",
    "Once downloaded, move the file to your working directory (val_images) and extract the dataset into the calib_data directory..\n",
    "Below code will read the images from val_image folder and create a calib_data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a4752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File organization complete.\n",
      "Creating calibration dataset complete.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.\n",
    "# SPDX-License-Identifier: MIT\n",
    "#\n",
    "# if len(sys.argv) < 3:\n",
    "#     print(\"Usage: python prepare_val_data.py <val_data_path> <calib_data_path>\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "source_folder = 'val_images'\n",
    "calib_data_path = 'calib_data'\n",
    "\n",
    "if not os.path.exists(source_folder):\n",
    "    print(\"The provided data path does not exist.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "files = os.listdir(source_folder)\n",
    "\n",
    "for filename in files:\n",
    "    if not filename.startswith('ILSVRC2012_val_') or not filename.endswith(\n",
    "            '.JPEG'):\n",
    "        continue\n",
    "\n",
    "    n_identifier = filename.split('_')[-1].split('.')[0]\n",
    "    folder_name = n_identifier\n",
    "    folder_path = os.path.join(source_folder, folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    file_path = os.path.join(source_folder, filename)\n",
    "    destination = os.path.join(folder_path, filename)\n",
    "    shutil.move(file_path, destination)\n",
    "\n",
    "print(\"File organization complete.\")\n",
    "\n",
    "if not os.path.exists(calib_data_path):\n",
    "    os.makedirs(calib_data_path)\n",
    "\n",
    "destination_folder = calib_data_path\n",
    "\n",
    "subfolders = os.listdir(source_folder)\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    source_subfolder = os.path.join(source_folder, subfolder)\n",
    "    destination_subfolder = os.path.join(destination_folder, subfolder)\n",
    "    os.makedirs(destination_subfolder, exist_ok=True)\n",
    "\n",
    "    files = os.listdir(source_subfolder)\n",
    "\n",
    "    if files:\n",
    "        file_to_copy = files[0]\n",
    "        source_file = os.path.join(source_subfolder, file_to_copy)\n",
    "        destination_file = os.path.join(destination_subfolder, file_to_copy)\n",
    "\n",
    "        shutil.copy(source_file, destination_file)\n",
    "\n",
    "print(\"Creating calibration dataset complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ae056f-a96c-47fb-b447-9de43e36e8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: The input ONNX model models\\resnet50.onnx can create InferenceSession successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QUARK_INFO]: Time information:\n",
      "2025-06-30 11:53:36.817912\n",
      "[QUARK_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- xhdnucstr12\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.26100\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 26 Model 36 Stepping 0, AuthenticAMD\n",
      "[QUARK_INFO]: Tools version information:\n",
      "                                        python --- 3.10.0\n",
      "                                          onnx --- 1.18.0\n",
      "                                   onnxruntime --- 1.22.0.dev20250626\n",
      "                                    quark.onnx --- 0.9+1a74724+1a74724\n",
      "[QUARK_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models\\resnet50.onnx\n",
      "                                  model_output --- models\\resnet50_quantized.onnx\n",
      "                       calibration_data_reader --- <utils_custom.ImageDataReader object at 0x0000014D15A5CA00>\n",
      "                         calibration_data_path --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                    extra_op_types_to_quantize --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                          subgraphs_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_npu_cnn --- True\n",
      "                        enable_npu_transformer --- False\n",
      "                     specific_tensor_precision --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- True\n",
      "                                    include_sq --- False\n",
      "                              include_rotation --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: Obtained calibration data with 9 iters\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Removed initializers from input\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Simplified model sucessfully\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Loading model...\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: The input ONNX model C:/Users/akumar23/AppData/Local/Temp/vai.simp.5bw9altr/model_simp.onnx can run inference successfully\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Start CrossLayerEqualization...\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: CrossLayerEqualization pattern num: 32\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Total CrossLayerEqualization steps: 1\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: CrossLayerEqualization Done.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: optimize the model for better hardware compatibility.\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: The opset version is 13 < 17. Skipping fusing layer normalization.\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: The opset version is 13 < 20. Skipping fusing Gelu.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Start calibration...\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Start collecting data, runtime depends on your model size and the number of calibration dataset.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Use all calibration data to calculate min mse\u001b[0m\n",
      "Computing range: 100%|███████████████████████████████████████████████████████████| 123/123 [01:13<00:00,  1.67tensor/s]\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Finished the calibration of PowerOfTwoMethod.MinMSE which costs 77.5s\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Rescale GlobalAveragePool /avgpool/GlobalAveragePool with factor 1.0048828125 to simulate DPU behavior.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Adjust the quantize info to meet the compiler constraints\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Input pos of pooling layer /avgpool/GlobalAveragePool is 1. Output pos of pooling layer /avgpool/GlobalAveragePool is 4.Modify opos from 4 to 1.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Adjust the quantize info to meet the compiler constraints\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: The operation types and their corresponding quantities of the input float model is shown in the table below.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model                    </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 53                             </span>│\n",
       "│ Relu                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 49                             </span>│\n",
       "│ MaxPool              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                              </span>│\n",
       "│ Add                  │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 16                             </span>│\n",
       "│ GlobalAveragePool    │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                              </span>│\n",
       "│ Flatten              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                              </span>│\n",
       "│ Gemm                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                              </span>│\n",
       "├──────────────────────┼────────────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models\\resnet50_quantized.onnx </span>│\n",
       "└──────────────────────┴────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model                   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m53                            \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Relu                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m49                            \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ MaxPool              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                             \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Add                  │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m16                            \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ GlobalAveragePool    │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                             \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Flatten              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                             \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gemm                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                             \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼────────────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels\\resnet50_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: The quantized information for all operation types is shown in the table below.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: The discrepancy between the operation types in the quantized model and the float model is due to the application of graph optimization.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type           </span>┃<span style=\"font-weight: bold\"> Activation </span>┃<span style=\"font-weight: bold\"> Weights  </span>┃<span style=\"font-weight: bold\"> Bias     </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ Conv              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> UINT8(53)  </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> INT8(53) </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> INT8(53) </span>│\n",
       "│ MaxPool           │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> UINT8(1)   </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│\n",
       "│ Add               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> UINT8(16)  </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│\n",
       "│ GlobalAveragePool │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> UINT8(1)   </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│\n",
       "│ Flatten           │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> UINT8(1)   </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│\n",
       "│ Gemm              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> UINT8(1)   </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> INT8(1)  </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> INT8(1)  </span>│\n",
       "└───────────────────┴────────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mWeights \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mBias    \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ Conv              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mUINT8(53) \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mINT8(53)\u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mINT8(53)\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ MaxPool           │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mUINT8(1)  \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Add               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mUINT8(16) \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ GlobalAveragePool │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mUINT8(1)  \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Flatten           │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mUINT8(1)  \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gemm              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mUINT8(1)  \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mINT8(1) \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mINT8(1) \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└───────────────────┴────────────┴──────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Quark Quantized model saved at: models\\resnet50_quantized.onnx\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Quark Quantization ---------------- #\n",
    "\n",
    "# Define dataset directory\n",
    "calib_dir = \"calib_data\" \n",
    "\n",
    "# Set input & output ONNX model paths\n",
    "input_model_path = tmp_model_path\n",
    "output_model_path = os.path.join(models_dir, \"resnet50_quantized.onnx\")\n",
    "\n",
    "# Preprocessing transformations\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "calib_dataset = torchvision.datasets.ImageFolder(root=calib_dir, transform=preprocess)\n",
    "\n",
    "#Data set \n",
    "num_calib_data = 54  \n",
    "calib_dataset = torch.utils.data.Subset(calib_dataset, range(num_calib_data))\n",
    "\n",
    "# Define DataLoader for Calibration\n",
    "calibration_dataloader = torch.utils.data.DataLoader(calib_dataset, batch_size=6, shuffle=False)\n",
    "\n",
    "# Configure Quark Quantization\n",
    "quant_config = get_default_config(\"XINT8\")  # Use XINT8 quantization  \n",
    "config = Config(global_quant_config=quant_config)\n",
    "\n",
    "# Create an ONNX Quantizer  \n",
    "quantizer = ModelQuantizer(config)  \n",
    "\n",
    "# Perform Quark Quantization  \n",
    "quant_model = quantizer.quantize_model(\n",
    "    model_input=input_model_path,   \n",
    "    model_output=output_model_path,   \n",
    "    calibration_data_reader=ImageDataReader(calibration_dataloader)  # Use ImageDataReader from utils_custom\n",
    ")\n",
    "\n",
    "print(f\"✅ Quark Quantized model saved at: {output_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46397081-c10c-4630-8299-563e81dea7b6",
   "metadata": {},
   "source": [
    "#### 4. Model inference on CPU / iGPU / NPU with single image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5cb104-5c6f-4043-a812-b5ee4a48af57",
   "metadata": {},
   "source": [
    "Now we have successfully quantized the model, and we will use the onnxruntime to do the inference on CPU, iGPU and NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b98ec05e-9b2e-4cc0-b3af-6d640c167074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:  (224, 224)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def load_labels(path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return np.asarray(data)\n",
    "\n",
    "def preprocess_image(input):\n",
    "    normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "  \n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        normalize,\n",
    "    ])\n",
    "    img_tensor = transform(input).unsqueeze(0)\n",
    "    return img_tensor.numpy()\n",
    "\n",
    "def softmax(x):\n",
    "    x = x.reshape(-1)\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def postprocess(result):\n",
    "    return softmax(np.array(result)).tolist()\n",
    "\n",
    "labels = load_labels('data/imagenet-simple-labels.json')\n",
    "image = Image.open('data/dog.jpg')\n",
    "\n",
    "print(\"Image size: \", image.size)\n",
    "input_data = preprocess_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db896e-fe9b-4167-9b0e-758040d50dd4",
   "metadata": {},
   "source": [
    "#### CPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd83dc38-b0ad-4001-bb8f-31b6b6f0cc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Final top prediction is: Golden Retriever\n",
      "----------------------------------------\n",
      "Inference time: 37.91 ms\n",
      "----------------------------------------\n",
      "------------ Top 5 labels are: ----------------------------\n",
      "['Golden Retriever' 'Labrador Retriever' 'Norwich Terrier'\n",
      " 'Curly-coated Retriever' 'Flat-Coated Retriever']\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run inference on CPU\n",
    "onnx_model_path = output_model_path\n",
    "cpu_options = onnxruntime.SessionOptions()\n",
    "\n",
    "cpu_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers=['CPUExecutionProvider'],\n",
    "    sess_options=cpu_options,\n",
    ")\n",
    "\n",
    "start = timer()\n",
    "cpu_outputs = cpu_session.run(None, {'input': input_data})\n",
    "end = timer()\n",
    "\n",
    "cpu_results = postprocess(cpu_outputs)\n",
    "inference_time = np.round((end - start) * 1000, 2)\n",
    "idx = np.argmax(cpu_results)\n",
    "\n",
    "print('----------------------------------------')\n",
    "print(f'Final top prediction is: {labels[idx]}')\n",
    "print('----------------------------------------')\n",
    "print(f'Inference time: {inference_time} ms')\n",
    "print('----------------------------------------')\n",
    "\n",
    "sort_idx = np.flip(np.squeeze(np.argsort(cpu_results)))\n",
    "print('------------ Top 5 labels are: ----------------------------')\n",
    "print(labels[sort_idx[:5]])\n",
    "print('-----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d267505-2078-44f0-9db2-e150b6e7af76",
   "metadata": {},
   "source": [
    "#### iGPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05b83ff1-63a1-40cf-b4a5-013857b42a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Final top prediction is: Golden Retriever\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Inference time: 101.22 ms\n",
      "----------------------------------------\n",
      "------------ Top 5 labels are: ----------------------------\n",
      "['Golden Retriever' 'Labrador Retriever' 'Norwich Terrier'\n",
      " 'Curly-coated Retriever' 'Flat-Coated Retriever']\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#iGPU inference\n",
    "dml_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Create Inference Session to run the quantized model on the iGPU\n",
    "dml_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['DmlExecutionProvider'],\n",
    "    provider_options = [{\"device_id\": \"0\"}]\n",
    ")\n",
    "start = time.time()\n",
    "dml_outputs = dml_session.run(None, {'input': input_data})\n",
    "end = time.time()\n",
    "\n",
    "dml_results = postprocess(dml_outputs)\n",
    "inference_time = np.round((end - start) * 1000, 2)\n",
    "idx = np.argmax(dml_results)\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('Final top prediction is: ' + labels[idx])\n",
    "print('----------------------------------------')\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('Inference time: ' + str(inference_time) + \" ms\")\n",
    "print('----------------------------------------')\n",
    "\n",
    "sort_idx = np.flip(np.squeeze(np.argsort(dml_results)))\n",
    "print('------------ Top 5 labels are: ----------------------------')\n",
    "print(labels[sort_idx[:5]])\n",
    "print('-----------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c9156d-6186-4256-9e3d-3826d6a3e0b8",
   "metadata": {},
   "source": [
    "#### NPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "128e098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the RYZEN_AI_INSTALLATION_PATH location.\n",
    "# Reader can find the installation path either under C:\\Program Files or the path defined at the time of installation.\n",
    "# example\n",
    "os.environ['RYZEN_AI_INSTALLATION_PATH']='C:\\Program Files\\RyzenAI\\1.5.0-0627'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122300c-c9c7-43b7-8e58-38a053a7f227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APU Type: STX\n",
      "C:\\Program Files\\RyzenAI\u0001.5.0-0627\n",
      "Setting xclbin file for STX\n",
      "C:\\Program Files\\RyzenAI\\1.5.0-0627\\voe-4.0-win_amd64\\xclbins\\strix\\AMD_AIE2P_4x4_Overlay.xclbin\n",
      "----------------------------------------\n",
      "Final top prediction is: Golden Retriever\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Inference time: 12.82 ms\n",
      "----------------------------------------\n",
      "------------ Top 5 labels are: ----------------------------\n",
      "['Golden Retriever' 'Labrador Retriever' 'Norwich Terrier'\n",
      " 'Curly-coated Retriever' 'Flat-Coated Retriever']\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#NPU inference\n",
    "\n",
    "# Before running, we need to set the ENV variable for the specific NPU we have\n",
    "# Run pnputil as a subprocess to enumerate PCI devices\n",
    "command = r'pnputil /enum-devices /bus PCI /deviceids '\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "# Check for supported Hardware IDs\n",
    "npu_type = ''\n",
    "if 'PCI\\\\VEN_1022&DEV_1502&REV_00' in stdout.decode(): npu_type = 'PHX/HPT'\n",
    "if 'PCI\\\\VEN_1022&DEV_17F0&REV_00' in stdout.decode(): npu_type = 'STX'\n",
    "if 'PCI\\\\VEN_1022&DEV_17F0&REV_10' in stdout.decode(): npu_type = 'STX'\n",
    "if 'PCI\\\\VEN_1022&DEV_17F0&REV_11' in stdout.decode(): npu_type = 'STX'\n",
    "\n",
    "print(f\"APU Type: {npu_type}\")\n",
    "\n",
    "install_dir = os.environ['RYZEN_AI_INSTALLATION_PATH']\n",
    "print(install_dir)\n",
    "\n",
    "match npu_type:\n",
    "    case 'PHX/HPT':\n",
    "        print(\"Setting provider options for PHX/HPT\")\n",
    "        xclbin_file = os.path.join(install_dir, 'voe-4.0-win_amd64', 'xclbins', 'phoenix', '4x4.xclbin')\n",
    "        provider_options = [{\n",
    "              'target': 'X1',\n",
    "              'xclbin': xclbin_file,\n",
    "              'ai_analyzer_visualization': True,\n",
    "              'ai_analyzer_profiling': True,\n",
    "          }]\n",
    "    case 'STX':\n",
    "        print(\"Setting provider options for STX\")\n",
    "        provider_options = [{\n",
    "              'ai_analyzer_visualization': True,\n",
    "              'ai_analyzer_profiling': True,\n",
    "          }]\n",
    "    case _:\n",
    "        print(\"Unrecognized APU type. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "npu_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['VitisAIExecutionProvider'],\n",
    "    provider_options = provider_options\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "npu_outputs = npu_session.run(None, {'input': input_data})\n",
    "end = time.time()\n",
    "\n",
    "npu_results = postprocess(npu_outputs)\n",
    "inference_time = np.round((end - start) * 1000, 2)\n",
    "idx = np.argmax(npu_results)\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('Final top prediction is: ' + labels[idx])\n",
    "print('----------------------------------------')\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('Inference time: ' + str(inference_time) + \" ms\")\n",
    "print('----------------------------------------')\n",
    "\n",
    "sort_idx = np.flip(np.squeeze(np.argsort(npu_results)))\n",
    "print('------------ Top 5 labels are: ----------------------------')\n",
    "print(labels[sort_idx[:5]])\n",
    "print('-----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b1bae-6afa-434a-9393-373a5aad3ac9",
   "metadata": {},
   "source": [
    "#### 5. Model Analysis on NPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5f148-094f-4e91-87c7-bce623a5af2e",
   "metadata": {},
   "source": [
    "After NPU inference, there are several '.json' files generated by the Ryzen AI tracing tool, which could be open by the AI Analyzer for further optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb04c788-f16a-4ce5-a7e8-6e414a45fda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!aianalyzer ./ -p 8001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9535c38b-4bb7-400a-bc75-be3b2018fe7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clone-0627",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
