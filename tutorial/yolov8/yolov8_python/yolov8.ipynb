{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with Image or Webcam on Ryzen AI\n",
    "\n",
    "This example demonstrates the object detection model inference on the embedded Neural Processing Unit (NPU) in your AMD Ryzen AI enabled PC with either single image or the live webcam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before starting, be sure you've installed the requirements listed in the requirements.txt file:\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Model from Ryzen AI model zoo\n",
    "The yolov8 model from [Ryzen AI model zoo](https://huggingface.co/amd) will be applied in this example. You may choose any other object detection models with tiny difference in the pre and post processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Notebook dependencies\n",
    "from huggingface_hub import hf_hub_download\n",
    "from yolov8_utils import get_directories\n",
    "\n",
    "current_dir = get_directories()\n",
    "\n",
    "# Download Yolov8 model from Ryzen AI model zoo. Registration is required before download.\n",
    "hf_hub_download(repo_id=\"amd/yolov8m\", filename=\"yolov8m.onnx\", local_dir=str(current_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model inference on CPU, iGPU and NPU with single image\n",
    "\n",
    "In this session, we will leverage the onnxruntime to do the inference on CPU, iGPU and NPU with single image.\n",
    "Here defines the pre and post processing functions which are the exactly the same on all OnnxRuntime EPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from yolov8_utils import *\n",
    "\n",
    "def preprocess(img):\n",
    "    img = torch.from_numpy(img)\n",
    "    img = img.float()  # uint8 to fp16/32\n",
    "    img /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    return img\n",
    "\n",
    "\n",
    "class DFL(nn.Module):\n",
    "    # Integral module of Distribution Focal Loss (DFL) proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
    "    def __init__(self, c1=16):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)\n",
    "        x = torch.arange(c1, dtype=torch.float)\n",
    "        self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))\n",
    "        self.c1 = c1\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, a = x.shape  # batch, channels, anchors\n",
    "        return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(\n",
    "            b, 4, a\n",
    "        )\n",
    "\n",
    "\n",
    "def dist2bbox(distance, anchor_points, xywh=True, dim=-1):\n",
    "    \"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\n",
    "    lt, rb = torch.split(distance, 2, dim)\n",
    "    x1y1 = anchor_points - lt\n",
    "    x2y2 = anchor_points + rb\n",
    "    if xywh:\n",
    "        c_xy = (x1y1 + x2y2) / 2\n",
    "        wh = x2y2 - x1y1\n",
    "        return torch.cat((c_xy, wh), dim)  # xywh bbox\n",
    "    return torch.cat((x1y1, x2y2), dim)  # xyxy bbox\n",
    "\n",
    "\n",
    "def post_process(x):\n",
    "    dfl = DFL(16)\n",
    "    anchors = torch.tensor(\n",
    "        np.load(\n",
    "            \"./anchors.npy\",\n",
    "            allow_pickle=True,\n",
    "        )\n",
    "    )\n",
    "    strides = torch.tensor(\n",
    "        np.load(\n",
    "            \"./strides.npy\",\n",
    "            allow_pickle=True,\n",
    "        )\n",
    "    )\n",
    "    box, cls = torch.cat([xi.view(x[0].shape[0], 144, -1) for xi in x], 2).split(\n",
    "        (16 * 4, 80), 1\n",
    "    )\n",
    "    dbox = dist2bbox(dfl(box), anchors.unsqueeze(0), xywh=True, dim=1) * strides\n",
    "    y = torch.cat((dbox, cls.sigmoid()), 1)\n",
    "    return y, x\n",
    "\n",
    "# Load labels of coco dataaset\n",
    "with open('coco.names', 'r') as f:\n",
    "        names = f.read()\n",
    "\n",
    "imgsz = [640, 640]\n",
    "\n",
    "# Set input image\n",
    "image_path = \"sample_yolov8.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Display the input image\n",
    "image = image.resize((640,640), Image.BICUBIC)\n",
    "display(image)\n",
    "\n",
    "# Load image\n",
    "dataset = LoadImages(\n",
    "    image_path, imgsz=imgsz, stride=32, auto=False, transforms=None, vid_stride=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU Inference\n",
    "\n",
    "Inference with onnxruntime on CPU ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Specify the path to the quantized ONNX Model\n",
    "onnx_model_path = \"yolov8m.onnx\"\n",
    "\n",
    "# Output file\n",
    "output_path = \"cpu_result.jpg\"\n",
    "\n",
    "cpu_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Create Inference Session to run the quantized model on the CPU\n",
    "cpu_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['CPUExecutionProvider'],\n",
    "    sess_options=cpu_options,\n",
    ")\n",
    "\n",
    "for batch in dataset:\n",
    "    path, im, im0s, vid_cap, s = batch\n",
    "    im = preprocess(im)\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]\n",
    "    start = time.time()\n",
    "    outputs = cpu_session.run(None, {cpu_session.get_inputs()[0].name: im.permute(0, 2, 3, 1).cpu().numpy()})\n",
    "    end = time.time()\n",
    "\n",
    "    inference_time = np.round((end - start) * 1000, 2)\n",
    "\n",
    "    # Postprocessing\n",
    "    outputs = [torch.tensor(item).permute(0, 3, 1, 2) for item in outputs]\n",
    "    preds = post_process(outputs)\n",
    "    preds = non_max_suppression(\n",
    "        preds, 0.25, 0.7, agnostic=False, max_det=300, classes=None\n",
    "    )\n",
    "    plot_images(\n",
    "        im,\n",
    "        *output_to_target(preds, max_det=15),\n",
    "        image_path,\n",
    "        fname=output_path,\n",
    "        names=names,\n",
    "    )\n",
    "    \n",
    "    print('----------------------------------------')\n",
    "    print('Inference time: ' + str(inference_time) + \" ms\")\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iGPU Inference\n",
    "\n",
    "We will leverage the onnxruntime DirectML ep to inference on AMD Radeon 780m iGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the quantized ONNX Model\n",
    "onnx_model_path = \"yolov8m.onnx\"\n",
    "\n",
    "# Output file\n",
    "output_path = \"dml_result.jpg\"\n",
    "\n",
    "dml_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Create Inference Session to run the quantized model on the iGPU\n",
    "dml_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['DmlExecutionProvider'],\n",
    "    provider_options = [{\"device_id\": \"0\"}]\n",
    ")\n",
    "\n",
    "for batch in dataset:\n",
    "    path, im, im0s, vid_cap, s = batch\n",
    "    im = preprocess(im)\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]\n",
    "    start = time.time()\n",
    "    outputs = dml_session.run(None, {dml_session.get_inputs()[0].name: im.permute(0, 2, 3, 1).cpu().numpy()})\n",
    "    end = time.time()\n",
    "\n",
    "    inference_time = np.round((end - start) * 1000, 2)\n",
    "\n",
    "    # Postprocessing\n",
    "    outputs = [torch.tensor(item).permute(0, 3, 1, 2) for item in outputs]\n",
    "    preds = post_process(outputs)\n",
    "    preds = non_max_suppression(\n",
    "        preds, 0.25, 0.7, agnostic=False, max_det=300, classes=None\n",
    "    )\n",
    "    plot_images(\n",
    "        im,\n",
    "        *output_to_target(preds, max_det=15),\n",
    "        image_path,\n",
    "        fname=output_path,\n",
    "        names=names,\n",
    "    )\n",
    "    \n",
    "    print('----------------------------------------')\n",
    "    print('Inference time: ' + str(inference_time) + \" ms\")\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NPU Inference\n",
    "\n",
    "Inference with onnxruntime on NPU ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
    "source": [
      "# Specify the path to the quantized ONNX Model\n",
      "onnx_model_path = \"yolov8m.onnx\"\n",
      "\n",
      "# Output file\n",
      "output_path = \"npu_result.jpg\"\n",
      "\n",
      "# Point to the config file path used for the VitisAI Execution Provider\n",
      "config_file_path = \"./vaip_config.json\"\n",
      "\n",
      "provider_options = [{\n",
      "              'config_file': config_file_path,\n",
      "              'ai_analyzer_visualization': True,\n",
      "              'ai_analyzer_profiling': True,\n",
      "          }]\n",
      "\n",
      "npu_session = onnxruntime.InferenceSession(\n",
      "    onnx_model_path,\n",
      "    providers = ['VitisAIExecutionProvider'],\n",
      "    provider_options = provider_options\n",
      ")\n",
      "\n",
      "for batch in dataset:\n",
      "    path, im, im0s, vid_cap, s = batch\n",
      "    im = preprocess(im)\n",
      "    if len(im.shape) == 3:\n",
      "        im = im[None]\n",
      "    start = time.time()\n",
      "    outputs = npu_session.run(None, {npu_session.get_inputs()[0].name: im.permute(0, 2, 3, 1).cpu().numpy()})\n",
      "    end = time.time()\n",
      "\n",
      "    inference_time = np.round((end - start) * 1000, 2)\n",
      "\n",
      "    # Postprocessing\n",
      "    outputs = [torch.tensor(item).permute(0, 3, 1, 2) for item in outputs]\n",
      "    preds = post_process(outputs)\n",
      "    preds = non_max_suppression(\n",
      "        preds, 0.25, 0.7, agnostic=False, max_det=300, classes=None\n",
      "    )\n",
      "    plot_images(\n",
      "        im,\n",
      "        *output_to_target(preds, max_det=15),\n",
      "        image_path,\n",
      "        fname=output_path,\n",
      "        names=names,\n",
      "    )\n",
      "    \n",
      "    print('----------------------------------------')\n",
      "    print('Inference time: ' + str(inference_time) + \" ms\")\n",
      "    print('----------------------------------------')"
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model inference on NPU with webcam\n",
    "\n",
    "Now we have validated the the model with image., and we will use the webcam as live input to do the inference on NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import cv2\n",
    "from yolov8_utils import *\n",
    "\n",
    "# display videos in notebook\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "\n",
    "def frame_process(frame, input_shape=(640, 640)):\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, input_shape)\n",
    "    img = torch.from_numpy(img)\n",
    "    img = img.float()  # uint8 to fp16/32\n",
    "    img /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return img\n",
    "    \n",
    "\n",
    "class DFL(nn.Module):\n",
    "    # Integral module of Distribution Focal Loss (DFL) proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
    "    def __init__(self, c1=16):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)\n",
    "        x = torch.arange(c1, dtype=torch.float)\n",
    "        self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))\n",
    "        self.c1 = c1\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, a = x.shape  # batch, channels, anchors\n",
    "        return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(\n",
    "            b, 4, a\n",
    "        )\n",
    "\n",
    "\n",
    "def dist2bbox(distance, anchor_points, xywh=True, dim=-1):\n",
    "    \"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\n",
    "    lt, rb = torch.split(distance, 2, dim)\n",
    "    x1y1 = anchor_points - lt\n",
    "    x2y2 = anchor_points + rb\n",
    "    if xywh:\n",
    "        c_xy = (x1y1 + x2y2) / 2\n",
    "        wh = x2y2 - x1y1\n",
    "        return torch.cat((c_xy, wh), dim)  # xywh bbox\n",
    "    return torch.cat((x1y1, x2y2), dim)  # xyxy bbox\n",
    "\n",
    "\n",
    "def post_process(x):\n",
    "    dfl = DFL(16)\n",
    "    anchors = torch.tensor(\n",
    "        np.load(\n",
    "            \"./anchors.npy\",\n",
    "            allow_pickle=True,\n",
    "        )\n",
    "    )\n",
    "    strides = torch.tensor(\n",
    "        np.load(\n",
    "            \"./strides.npy\",\n",
    "            allow_pickle=True,\n",
    "        )\n",
    "    )\n",
    "    box, cls = torch.cat([xi.view(x[0].shape[0], 144, -1) for xi in x], 2).split(\n",
    "        (16 * 4, 80), 1\n",
    "    )\n",
    "    dbox = dist2bbox(dfl(box), anchors.unsqueeze(0), xywh=True, dim=1) * strides\n",
    "    y = torch.cat((dbox, cls.sigmoid()), 1)\n",
    "    return y, x\n",
    "\n",
    "# Load labels of coco dataaset\n",
    "with open('coco.names', 'r') as f:\n",
    "        names = f.read()\n",
    "\n",
    "imgsz = [640, 640]\n",
    "\n",
    "# Specify the path to the quantized ONNZ Model\n",
    "onnx_model_path = \"yolov8m.onnx\"\n",
    "\n",
    "# Point to the config file path used for the VitisAI Execution Provider\n",
    "config_file_path = \"./vaip_config.json\"\n",
    "\n",
    "npu_options = onnxruntime.SessionOptions()\n",
    "\n",
    "npu_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['VitisAIExecutionProvider'],\n",
    "    sess_options=npu_options,\n",
    "    provider_options=[{'config_file': config_file_path}]\n",
    ")\n",
    "\n",
    "with open('coco.names', 'r') as f:\n",
    "        names = f.read()\n",
    "\n",
    "# Video input\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while (True):\n",
    "    try:\n",
    "        clear_output(wait=True)\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        input_shape = (640, 640)\n",
    "\n",
    "        im = frame_process(frame, input_shape)\n",
    "        if len(im.shape) == 3:\n",
    "            im = im[None]\n",
    "        outputs = npu_session.run(None, {npu_session.get_inputs()[0].name: im.permute(0, 2, 3, 1).cpu().numpy()})\n",
    "\n",
    "        # Postprocessing\n",
    "        outputs = [torch.tensor(item).permute(0, 3, 1, 2) for item in outputs]\n",
    "        preds = post_process(outputs)\n",
    "        preds = non_max_suppression(\n",
    "            preds, 0.25, 0.7, agnostic=False, max_det=300, classes=None\n",
    "        )\n",
    "\n",
    "        colors = [[random.randint(0, 255) for _ in range(3)] \n",
    "                for _ in range(len(names))]\n",
    "\n",
    "        plot_images(\n",
    "        im,\n",
    "        *output_to_target(preds, max_det=15),\n",
    "        frame,\n",
    "        fname=\"output.jpg\",\n",
    "        names=names,\n",
    "        )\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model Analysis on NPU\r\n",
    "After NPU inference, there are several '.json' files generated by the Ryzen AI tracing tool, which could be open by the AI Analyzer for further optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aianalyzer ./ -p 8001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ryzen-ai-1.2.0",
   "language": "python",
   "name": "ryzen-ai-1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
