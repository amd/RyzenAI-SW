{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World Example\n",
    "\n",
    "This is a simple Jupyter Notebook that walks through the 4 steps of compiling and running a PyTorch model on the embedded Neural Processing Unit (NPU) in your AMD Ryzen AI enabled PC. The steps are as follows:\n",
    "\n",
    "1. Get model - download or create a PyTorch model that we will run on the NPU\n",
    "2. Export to ONNX - convert the PyTorch model to ONNX format.\n",
    "3. Quantize - optimize the model for faster inference on the NPU by reducing its precision to INT8.\n",
    "4. Run Model on CPU and NPU - compare performance between running the model on the CPU and on the NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from -r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from -r requirements.txt (line 2)) (6.29.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2024.6.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (1.8.5)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (8.26.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (8.6.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (6.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (26.1.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (5.14.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (1.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 2)) (2.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 2)) (4.2.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 2)) (306)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\vgods\\miniconda3\\envs\\ryzen-ai-1.2.0\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "# Before starting, be sure you've installed the requirements listed in the requirements.txt file:\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports & Environment Variables\n",
    "\n",
    "We'll use the following imports in our example. `torch` and `torch_nn` are used for building and running ML models. We'll use them to define a small neural network and to generate the model weights. `os` is used for interacting with the operating system and is used to manage our environment variables, file paths, and directories. `subprocess` allows us to retrieve the hardware information. `onnx` and `onnxruntime` are used to work with our model in the ONNX format and for running our inference. `vai_q_onnx` is part of the Vitis AI Quantizer for ONNX models. We use it to perform quantization, converting the model into an INT8 format that is optimized for the NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import subprocess\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import onnx\n",
    "import shutil\n",
    "from timeit import default_timer as timer\n",
    "#import vai_q_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well, we want to set the environment variables based on the NPU device we have in our PC. For more information about NPU configurations, see: For more information about NPU configurations, refer to the official [AMD Ryzen AI Documentation](https://ryzenai.docs.amd.com/en/latest/runtime_setup.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPU Type: KRK\n"
     ]
    }
   ],
   "source": [
    "# This function detects the APU (NPU) type in your system to configure environment variables for hardware-specific optimization.\n",
    "def get_npu_info():\n",
    "    # Run pnputil as a subprocess to enumerate PCI devices\n",
    "    command = r'pnputil /enum-devices /bus PCI /deviceids '\n",
    "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "    # Check for supported Hardware IDs\n",
    "    npu_type = ''\n",
    "    if 'PCI\\\\VEN_1022&DEV_1502&REV_00' in stdout.decode(): npu_type = 'PHX/HPT'\n",
    "    if 'PCI\\\\VEN_1022&DEV_17F0&REV_00' in stdout.decode(): npu_type = 'STX'\n",
    "    if 'PCI\\\\VEN_1022&DEV_17F0&REV_10' in stdout.decode(): npu_type = 'STX'\n",
    "    if 'PCI\\\\VEN_1022&DEV_17F0&REV_11' in stdout.decode(): npu_type = 'STX'\n",
    "    if 'PCI\\\\VEN_1022&DEV_17F0&REV_20' in stdout.decode(): npu_type = 'KRK'\n",
    "    return npu_type\n",
    "\n",
    "npu_type = get_npu_info()\n",
    "print(f\"APU Type: {npu_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for STX/KRK\n",
      "XLNX_VART_FIRMWARE= C:\\Program Files\\RyzenAI\\1.6.0\\voe-4.0-win_amd64\\xclbins\\strix\\AMD_AIE2P_4x4_Overlay.xclbin\n",
      "NUM_OF_DPU_RUNNERS= 1\n",
      "XLNX_TARGET_NAME= AMD_AIE2_Nx4_Overlay\n"
     ]
    }
   ],
   "source": [
    "# XLNX_VART_FIRMWARE - Specifies the firmware file used by the NPU for runtime execution\n",
    "# NUM_OF_DPU_RUNNERS - Specifies the number of DPU runners (processing cores) available for execution\n",
    "# XLNX_TARGET_NAME - Name of the target hardware configuration\n",
    "\n",
    "def set_environment_variable(npu_type):\n",
    "\n",
    "    install_dir = os.environ['RYZEN_AI_INSTALLATION_PATH']\n",
    "    match npu_type:\n",
    "        case 'PHX/HPT':\n",
    "            print(\"Setting environment for PHX/HPT\")\n",
    "            os.environ['XLNX_VART_FIRMWARE']= os.path.join(install_dir, 'voe-4.0-win_amd64', 'xclbins', 'phoenix', '4x4.xclbin')\n",
    "            os.environ['NUM_OF_DPU_RUNNERS']='1'\n",
    "            os.environ['XLNX_TARGET_NAME']='AMD_AIE2_Nx4_Overlay'\n",
    "        case 'STX' | 'KRK':\n",
    "            print(\"Setting environment for STX/KRK\")\n",
    "            os.environ['XLNX_VART_FIRMWARE']= os.path.join(install_dir, 'voe-4.0-win_amd64', 'xclbins', 'strix', 'AMD_AIE2P_4x4_Overlay.xclbin')\n",
    "            os.environ['NUM_OF_DPU_RUNNERS']='1'\n",
    "            os.environ['XLNX_TARGET_NAME']='AMD_AIE2_Nx4_Overlay'\n",
    "        case _:\n",
    "            print(\"Unrecognized APU type. Exiting.\")\n",
    "            exit()\n",
    "    print('XLNX_VART_FIRMWARE=', os.environ['XLNX_VART_FIRMWARE'])\n",
    "    print('NUM_OF_DPU_RUNNERS=', os.environ['NUM_OF_DPU_RUNNERS'])\n",
    "    print('XLNX_TARGET_NAME=', os.environ['XLNX_TARGET_NAME'])\n",
    "\n",
    "set_environment_variable(npu_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Model\n",
    "Here, we'll use the PyTorch library to define and instantiate a simple neural network model called `SmallModel` as a starting point. You can swap this model with any custom model, but make sure the input/output shapes remain compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmallModel(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "class SmallModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x) \n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x) \n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x) \n",
    "        \n",
    "        x = torch.add(x, 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "pytorch_model = SmallModel()\n",
    "\n",
    "pytorch_model.eval()\n",
    "\n",
    "# Print the model architecture\n",
    "print(pytorch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Export to ONNX\n",
    "\n",
    "The following code is used for exporting a PyTorch model (pytorch_model) to the ONNX (Open Neural Network Exchange) format. ONNX is an open format that facilitates interoperability between different AI frameworks. Ryzen AI uses ONNX as the input format for quantization using the Vitis AI Quantizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kfreidank\\AppData\\Local\\Temp\\ipykernel_18628\\2861012411.py:13: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W1213 23:10:49.984000 18628 site-packages\\torch\\onnx\\_internal\\exporter\\_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W1213 23:10:51.139000 18628 site-packages\\torch\\onnx\\_internal\\exporter\\_registration.py:107] torchvision is not installed. Skipping torchvision::nms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `SmallModel([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `SmallModel([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 17},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.9.1+cpu',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"input\"<FLOAT,[s77,3,224,224]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"output\"<FLOAT,[1,256,224,224]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"conv1.weight\"<FLOAT,[32,3,3,3]>{TorchTensor(...)},\n",
       "                %\"conv1.bias\"<FLOAT,[32]>{TorchTensor(...)},\n",
       "                %\"conv2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"conv3.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"conv4.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"conv2.weight\"<FLOAT,[64,32,3,3]>{TorchTensor(...)},\n",
       "                %\"conv3.weight\"<FLOAT,[128,64,3,3]>{TorchTensor(...)},\n",
       "                %\"conv4.weight\"<FLOAT,[256,128,3,3]>{TorchTensor(...)},\n",
       "                %\"scalar_tensor_default\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name='scalar_tensor_default')}\n",
       "            ),\n",
       "        ) {\n",
       "            0 |  # node_conv2d\n",
       "                 %\"conv2d\"<FLOAT,[s77,32,224,224]> ⬅️ ::Conv(%\"input\", %\"conv1.weight\"{...}, %\"conv1.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            1 |  # node_relu\n",
       "                 %\"relu\"<FLOAT,[s77,32,224,224]> ⬅️ ::Relu(%\"conv2d\")\n",
       "            2 |  # node_conv2d_1\n",
       "                 %\"conv2d_1\"<FLOAT,[s77,64,224,224]> ⬅️ ::Conv(%\"relu\", %\"conv2.weight\"{...}, %\"conv2.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            3 |  # node_relu_1\n",
       "                 %\"relu_1\"<FLOAT,[s77,64,224,224]> ⬅️ ::Relu(%\"conv2d_1\")\n",
       "            4 |  # node_conv2d_2\n",
       "                 %\"conv2d_2\"<FLOAT,[s77,128,224,224]> ⬅️ ::Conv(%\"relu_1\", %\"conv3.weight\"{...}, %\"conv3.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            5 |  # node_relu_2\n",
       "                 %\"relu_2\"<FLOAT,[s77,128,224,224]> ⬅️ ::Relu(%\"conv2d_2\")\n",
       "            6 |  # node_conv2d_3\n",
       "                 %\"conv2d_3\"<FLOAT,[s77,256,224,224]> ⬅️ ::Conv(%\"relu_2\", %\"conv4.weight\"{...}, %\"conv4.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            7 |  # node_relu_3\n",
       "                 %\"relu_3\"<FLOAT,[s77,256,224,224]> ⬅️ ::Relu(%\"conv2d_3\")\n",
       "            8 |  # node_add_40\n",
       "                 %\"output\"<FLOAT,[1,256,224,224]> ⬅️ ::Add(%\"relu_3\", %\"scalar_tensor_default\"{1.0})\n",
       "            return %\"output\"<FLOAT,[1,256,224,224]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_conv1_weight: \"f32[32, 3, 3, 3]\", p_conv1_bias: \"f32[32]\", p_conv2_weight: \"f32[64, 32, 3, 3]\", p_conv2_bias: \"f32[64]\", p_conv3_weight: \"f32[128, 64, 3, 3]\", p_conv3_bias: \"f32[128]\", p_conv4_weight: \"f32[256, 128, 3, 3]\", p_conv4_bias: \"f32[256]\", x: \"f32[s77, 3, 224, 224]\"):\n",
       "                     # File: c:\\Users\\kfreidank\\miniforge3\\envs\\ryzen-hello\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d: \"f32[s77, 32, 224, 224]\" = torch.ops.aten.conv2d.default(x, p_conv1_weight, p_conv1_bias, [1, 1], [1, 1]);  x = p_conv1_weight = p_conv1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\kfreidank\\miniforge3\\envs\\ryzen-hello\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu: \"f32[s77, 32, 224, 224]\" = torch.ops.aten.relu.default(conv2d);  conv2d = None\n",
       "            \n",
       "                     # File: c:\\Users\\kfreidank\\miniforge3\\envs\\ryzen-hello\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_1: \"f32[s77, 64, 224, 224]\" = torch.ops.aten.conv2d.default(relu, p_conv2_weight, p_conv2_bias, [1, 1], [1, 1]);  relu = p_conv2_weight = p_conv2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\kfreidank\\miniforge3\\envs\\ryzen-hello\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_1: \"f32[s77, 64, 224, 224]\" = torch.ops.aten.relu.default(conv2d_1);  conv2d_1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\kfreidank\\miniforge3\\envs\\ryzen-hello\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_2: \"f32[s77, 128, 224, 224]\" = torch.ops.aten.conv2d.default(relu_1, p_conv3_weight, p_conv3_bias, [1, 1], [1, 1]);  relu_1 = p_conv3_weight = p_conv3_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\kfreidank\\miniforge3\\envs\\ryzen-hello\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_2: \"f32[s77, 128, 224, 224]\" = torch.ops.aten.relu.default(conv2d_2);  conv2d_2 = None\n",
       "            \n",
       "                     # File: c:\\Users\\kfreidank\\miniforge3\\envs\\ryzen-hello\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_3: \"f32[s77, 256, 224, 224]\" = torch.ops.aten.conv2d.default(relu_2, p_conv4_weight, p_conv4_bias, [1, 1], [1, 1]);  relu_2 = p_conv4_weight = p_conv4_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\kfreidank\\miniforge3\\envs\\ryzen-hello\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_3: \"f32[s77, 256, 224, 224]\" = torch.ops.aten.relu.default(conv2d_3);  conv2d_3 = None\n",
       "            \n",
       "                     # File: C:\\Users\\kfreidank\\AppData\\Local\\Temp\\ipykernel_18628\\666140956.py:25 in forward, code: x = torch.add(x, 1)\n",
       "                    scalar_tensor_default: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    add_40: \"f32[1, 256, 224, 224]\" = torch.ops.aten.add.Tensor(relu_3, scalar_tensor_default);  relu_3 = scalar_tensor_default = None\n",
       "                    return (add_40,)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_conv1_weight: PARAMETER target='conv1.weight'\n",
       "            p_conv1_bias: PARAMETER target='conv1.bias'\n",
       "            p_conv2_weight: PARAMETER target='conv2.weight'\n",
       "            p_conv2_bias: PARAMETER target='conv2.bias'\n",
       "            p_conv3_weight: PARAMETER target='conv3.weight'\n",
       "            p_conv3_bias: PARAMETER target='conv3.bias'\n",
       "            p_conv4_weight: PARAMETER target='conv4.weight'\n",
       "            p_conv4_bias: PARAMETER target='conv4.bias'\n",
       "            x: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            add_40: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {s77: VR[0, int_oo]}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate dummy input data\n",
    "batch_size = 1\n",
    "input_channels = 3\n",
    "input_size = 224\n",
    "dummy_input = torch.rand(batch_size, input_channels, input_size, input_size)\n",
    "\n",
    "# Prep for ONNX export\n",
    "inputs = {\"x\": dummy_input}\n",
    "dynamic_axes = {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "tmp_model_path = \"models/helloworld.onnx\"\n",
    "\n",
    "# Call export function\n",
    "torch.onnx.export(\n",
    "        pytorch_model,\n",
    "        dummy_input,\n",
    "        tmp_model_path,\n",
    "        export_params=True,\n",
    "        opset_version=17,  # Recommended opset\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quantize Model\n",
    "\n",
    "Using the static quantization method provided by the AMD Quark Quantizer and providing the newly exported ONNX model, we'll quantize the model to INT8. Quantization reduces the precision of model weights and activations from 32-bit floating point (FP32) to 8-bit integers (INT8). This compression allows the model to run faster on hardware accelerators like NPUs, while maintaining nearly the same accuracy. For more information on this quantization method, see [AMD Quark Quantization](https://ryzenai.docs.amd.com/en/latest/modelport.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: Checking custom ops library ...\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: The CPU version of custom ops library already exists.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Checked custom ops library.\u001b[0m\n",
      "c:\\Users\\kfreidank\\miniforge3\\envs\\ryzen-hello\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: The input ONNX model can create InferenceSession successfully\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Random input name input shape [1, 3, 224, 224] type <class 'numpy.float32'> \u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Obtained calibration data with 1 iters\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The configuration of the quantization is Config(global_quant_config=QuantizationConfig(calibrate_method=<PowerOfTwoMethod.MinMSE: 1>, quant_format=<QuantFormat.QDQ: 1>, activation_type=<QuantType.QUInt8: 1>, weight_type=<QuantType.QInt8: 0>, input_nodes=[], output_nodes=[], op_types_to_quantize=[], nodes_to_quantize=[], extra_op_types_to_quantize=[], nodes_to_exclude=[], subgraphs_to_exclude=[], specific_tensor_precision=False, execution_providers=['CPUExecutionProvider'], per_channel=False, reduce_range=False, optimize_model=True, use_dynamic_quant=False, use_external_data_format=False, convert_fp16_to_fp32=False, convert_nchw_to_nhwc=False, include_sq=False, include_rotation=False, include_cle=True, include_auto_mp=False, include_fast_ft=False, enable_npu_cnn=True, enable_npu_transformer=False, debug_mode=False, crypto_mode=False, print_summary=True, ignore_warnings=True, log_severity_level=1, extra_options={'ActivationSymmetric': True, 'UseRandomData': True}))\n",
      "[QUARK_INFO]: Time information:\n",
      "2025-12-13 23:11:15.395783\n",
      "[QUARK_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- windel\n",
      "                                       release --- 11\n",
      "                                       version --- 10.0.26200\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 26 Model 96 Stepping 0, AuthenticAMD\n",
      "[QUARK_INFO]: Tools version information:\n",
      "                                        python --- 3.12.11\n",
      "                                          onnx --- 1.18.0\n",
      "                                   onnxruntime --- 1.23.0.dev20250928\n",
      "                                    quark.onnx --- 0.10+db671e3+db671e3\n",
      "[QUARK_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/helloworld.onnx\n",
      "                                  model_output --- models/helloworld_quantized.onnx\n",
      "                       calibration_data_reader --- None\n",
      "                         calibration_data_path --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                    extra_op_types_to_quantize --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                          subgraphs_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_npu_cnn --- True\n",
      "                        enable_npu_transformer --- False\n",
      "                     specific_tensor_precision --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- True\n",
      "                                    include_sq --- False\n",
      "                              include_rotation --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True, 'UseRandomData': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: Removed initializers from input\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Simplified model sucessfully\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Loading model...\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: The input ONNX model can run inference successfully\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Start CrossLayerEqualization...\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: CrossLayerEqualization pattern num: 3\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Total CrossLayerEqualization steps: 1\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: CrossLayerEqualization Done.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: optimize the model for better hardware compatibility.\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: The opset version is 17 < 20. Skipping fusing Gelu.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Start calibration...\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Start collecting data, runtime depends on your model size and the number of calibration dataset.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Use all calibration data to calculate min mse\u001b[0m\n",
      "Computing range: 100%|██████████| 10/10 [00:05<00:00,  1.95tensor/s]\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Finished the calibration of PowerOfTwoMethod.MinMSE which costs 5.4s\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Adjust the quantize info to meet the compiler constraints\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: The operation types and their corresponding quantities of the input float model is shown in the table below.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model                      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 4                                </span>│\n",
       "│ Relu                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 4                                </span>│\n",
       "│ Add                  │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                                </span>│\n",
       "├──────────────────────┼──────────────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/helloworld_quantized.onnx </span>│\n",
       "└──────────────────────┴──────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model                     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m4                               \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Relu                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m4                               \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Add                  │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                               \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼──────────────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/helloworld_quantized.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴──────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: The quantized information for all operation types is shown in the table below.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: The discrepancy between the operation types in the quantized model and the float model is due to the application of graph optimization.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type </span>┃<span style=\"font-weight: bold\"> Activation </span>┃<span style=\"font-weight: bold\"> Weights </span>┃<span style=\"font-weight: bold\"> Bias    </span>┃\n",
       "┡━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ Conv    │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> UINT8(4)   </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> INT8(4) </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> INT8(4) </span>│\n",
       "│ Add     │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> UINT8(1)   </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">         </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">         </span>│\n",
       "└─────────┴────────────┴─────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mWeights\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mBias   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ Conv    │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mUINT8(4)  \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mINT8(4)\u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mINT8(4)\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Add     │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mUINT8(1)  \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└─────────┴────────────┴─────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated and quantized model saved at: models/helloworld_quantized.onnx\n"
     ]
    }
   ],
   "source": [
    "from quark.onnx.quantization.config import Config, get_default_config\n",
    "from quark.onnx import ModelQuantizer\n",
    "\n",
    "# `input_model_path` is the path to the original, unquantized ONNX model.\n",
    "input_model_path = \"models/helloworld.onnx\"\n",
    "\n",
    "# `output_model_path` is the path where the quantized model will be saved.\n",
    "output_model_path = \"models/helloworld_quantized.onnx\"\n",
    "\n",
    "# Use default quantization configuration\n",
    "quant_config = get_default_config(\"XINT8\")\n",
    "quant_config.extra_options[\"UseRandomData\"] = True\n",
    "# Defines the quantization configuration for the whole model\n",
    "config = Config(global_quant_config=quant_config)\n",
    "print(\"The configuration of the quantization is {}\".format(config))\n",
    "\n",
    "# Create an ONNX Quantizer\n",
    "quantizer = ModelQuantizer(config)\n",
    "\n",
    "# Quantize the ONNX model\n",
    "quant_model = quantizer.quantize_model(model_input = input_model_path,\n",
    "                                       model_output = output_model_path,\n",
    "                                       calibration_data_path = None)\n",
    "\n",
    "print('Calibrated and quantized model saved at:', output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Model\n",
    "\n",
    "#### CPU Run\n",
    "\n",
    "Before runnning the model on the NPU, we'll run the model on the CPU and get the execution time for comparison with the NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the quantized ONNZ Model\n",
    "quantized_model_path = r'./models/helloworld_quantized.onnx'\n",
    "model = onnx.load(quantized_model_path)\n",
    "\n",
    "# Create some random input data for testing\n",
    "input_data = np.random.uniform(low=-1, high=1, size=(batch_size, input_channels, input_size, input_size)).astype(np.float32)\n",
    "\n",
    "cpu_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Create Inference Session to run the quantized model on the CPU\n",
    "cpu_session = onnxruntime.InferenceSession(\n",
    "    model.SerializeToString(),\n",
    "    providers = ['CPUExecutionProvider'],\n",
    "    sess_options=cpu_options,\n",
    ")\n",
    "\n",
    "# Run Inference\n",
    "start = timer()\n",
    "cpu_results = cpu_session.run(None, {'input': input_data})\n",
    "cpu_total = timer() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NPU Run\n",
    "\n",
    "Now, we'll run it on the NPU and time the execution so that we can compare the results with the CPU.\n",
    "If the model has already been compiled, it won't recompile unless you delete the generated cache folder using the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'c:\\Users\\kfreidank\\projects\\amd_demos\\RyzenAI-SW\\tutorial\\hello_world\\cache\\hello_cache' does not exist.\n"
     ]
    }
   ],
   "source": [
    "# We want to make sure we compile everytime, otherwise the tools will use the cached version\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "directory_path = os.path.join(current_directory,  r'cache\\hello_cache')\n",
    "cache_directory = os.path.join(current_directory,  r'cache')\n",
    "\n",
    "# Check if the directory exists and delete it if it does\n",
    "if os.path.exists(directory_path):\n",
    "    shutil.rmtree(directory_path)\n",
    "    print(f\"Directory deleted successfully. Starting Fresh.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and run\n",
    "\n",
    "On the first run, the model will compile for the NPU before executing the inference. It's best to run the following cell again if you want to see better inference times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_dir = os.environ['RYZEN_AI_INSTALLATION_PATH']\n",
    "config_file_path = os.path.join(install_dir, 'voe-4.0-win_amd64', 'vaip_config.json') # Path to the NPU config file\n",
    "xclbin_file = ''\n",
    "provider_options = []\n",
    "match npu_type:\n",
    "    case 'PHX/HPT':\n",
    "        print(\"Setting xclbin file for PHX/HPT\")\n",
    "        xclbin_file = os.path.join(install_dir, 'voe-4.0-win_amd64', 'xclbins', 'phoenix', '4x4.xclbin')\n",
    "        provider_options = [{\n",
    "                        'target': 'X1',\n",
    "                        'xclbin': xclbin_file,\n",
    "                        'log_level':'info',\n",
    "                    }]\n",
    "    case 'STX' | 'KRK':\n",
    "        provider_options = [{\n",
    "                'log_level':'info',\n",
    "            }]\n",
    "    case _:\n",
    "        print(\"Unrecognized APU type. Exiting.\")\n",
    "        exit()\n",
    "aie_options = onnxruntime.SessionOptions()\n",
    "\n",
    "aie_session = onnxruntime.InferenceSession(\n",
    "    model.SerializeToString(),\n",
    "    providers=['VitisAIExecutionProvider'],\n",
    "    sess_options=aie_options,\n",
    "    provider_options = provider_options\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Inference\n",
    "start = timer()\n",
    "npu_results = aie_session.run(None, {'input': input_data})\n",
    "npu_total = timer() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather our results and see what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Execution Time: 0.17882769999999937\n",
      "NPU Execution Time: 0.20666400000001772\n"
     ]
    }
   ],
   "source": [
    "print(f\"CPU Execution Time: {cpu_total}\")\n",
    "print(f\"NPU Execution Time: {npu_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For a model this small in size, you likely won't see much of a performance gain when using the NPU versus the CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at running the model on the NPU lots of times so that we can see the NPU being utilized.\n",
    "To do this, make sure to have Task Manager opened in a window you can see when you run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 50 iterations of a small model:\n",
      "- CPU Execution Time: 8.32508400000006\n",
      "- NPU Execution Time: 8.485271200000227\n"
     ]
    }
   ],
   "source": [
    "iterations = 50 # edit this for more or less\n",
    "\n",
    "npu_total = cpu_total = 0\n",
    "for i in range(iterations):\n",
    "    start = timer()\n",
    "    npu_results = aie_session.run(None, {'input': input_data})\n",
    "    npu_total += timer() - start\n",
    "    start = timer()\n",
    "    cpu_results = cpu_session.run(None, {'input': input_data})\n",
    "    cpu_total += timer() - start\n",
    "\n",
    "print(f\"For {iterations} iterations of a small model:\")\n",
    "print(f\"- CPU Execution Time: {cpu_total}\")\n",
    "print(f\"- NPU Execution Time: {npu_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it. Your first model running on the NPU. We recommend trying a more complex model like ResNet50 or a custom model to compare performance and accuracy on the NPU.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ryzen-hello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
